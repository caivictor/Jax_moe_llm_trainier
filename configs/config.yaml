# Central configuration file for the training pipeline

# Data Configuration
data:
  dataset_name: "HuggingFaceFW/fineweb" # Or specific subset like FineWeb-Edu
  dataset_config_name: null # Specify if needed (e.g., 'sample-10BT')
  tokenizer_name: "gpt2" # Specify desired Hugging Face tokenizer
  max_seq_length: 4096 # Target context window (Reduced from 1M to fit memory)
  preprocessing_num_workers: 8 # Number of workers for data preprocessing
  streaming: True # Use streaming for large datasets like FineWeb
  batch_size_per_device: 1 # Adjust based on GPU memory
  shuffle_buffer_size: 10 #10000
  # Define number of samples for validation when splitting train set
  num_validation_samples: 50000 # e.g., 50 eval steps * 1 batch/device * 8 devices * ~12 samples/batch? Adjust as needed.

# Model Configuration
model:
  vocab_size: 50257 # Example for GPT-2 tokenizer, adjust based on actual tokenizer
  d_model: 1024 # Example dimension, adjust for ~6B params (Reduced from 2048)
  num_layers: 4 # Example layer count
  num_heads: 4 # Example head count
  d_ff: 4096 # Feed-forward dimension (Reduced from 8192, 4*d_model)
  dropout_rate: 0.1
  # MoE Specific Configuration
  use_moe: True
  moe_layer_freq: 1 # 2 # Apply MoE every N layers
  num_experts: 4 # Number of experts per MoE layer (Reduced from 8)
  num_experts_per_token: 2 # Number of experts to route each token to (Top-K)
  capacity_factor: 1.25 # Renamed from expert_capacity_factor - Controls expert buffer size
  router_z_loss_coef: 0.001 # Auxiliary load balancing loss coefficient
  # Positional Encoding for Long Context
  positional_encoding_type: "rope" # Rotary Positional Embeddings
  rope_theta: 10000.0 # RoPE parameter
  # Add params for YaRN or other interpolation if needed

# Optimizer Configuration (Shampoo)
optimizer:
  name:  "AdamW" #"shampoo"
  learning_rate: 1e-4
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999 # Or specific Shampoo betas if different
  # Shampoo Specific Hyperparameters (refer to optax-shampoo or chosen implementation)
  block_size: 1024
  preconditioning_compute_steps: 10 # Frequency of preconditioner update
  graft_type: 0 # Grafting type (0: None, 1: SGD, 2: AdaGrad, 3: RMSProp, 4: Adam)
  nesterov: False # Whether to use Nesterov momentum

# Training Configuration
training:
  output_dir: "./output"
  num_train_steps: 1000000 # Total training steps
  eval_steps: 5000 # Evaluate every N steps
  eval_steps_limit: 50 # Number of batches to use for each evaluation run
  log_steps: 100 # Log metrics every N steps
  save_steps: 1000 # Checkpoint every N steps (as per requirement)
  seed: 42
  # Distributed Training
  use_distributed: True # Flag to enable/disable distributed setup
  # num_devices: 8 # Example: Number of GPUs/TPUs - Determined automatically by JAX

# Checkpointing Configuration (Orbax)
checkpointing:
  checkpoint_dir: "./checkpoints"
  keep: 3 # Number of recent checkpoints to keep
  overwrite: False
  async_checkpointing: True # Enable asynchronous saving

# Logging Configuration
logging:
  use_tensorboard: True
  use_wandb: False
  wandb_project: "jax_moe_llm"
  wandb_entity: null
